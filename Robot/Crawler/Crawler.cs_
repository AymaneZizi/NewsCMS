using System;
using System.Collections.Generic;
using System.Configuration;
using System.IO;
using System.Net;
using System.Text;
using System.Linq;
using System.Text.RegularExpressions;
using SHDocVw;
using CrawlerEngine;
using System.Threading;

namespace CrawlerEngine
{
    public static class _Crawler
    {
        #region Private Fields

        private static List<Page> _pages = new List<Page>();
        private static List<string> _externalUrls = new List<string>();
        private static List<string> _rssUrls = new List<string>();
        private static List<string> _otherUrls = new List<string>();
        private static List<string> _failedUrls = new List<string>();
        private static List<string> _exceptions = new List<string>();
        private static List<string> _classes = new List<string>();
        private static StringBuilder _logBuffer = new StringBuilder();
        public static Sites BaseSite;
        public static int SpamCount = 0;
        private static int SelfLink = 0;
        private static bool StopCrawling = false;
        static TazehaContext entiti = new TazehaContext();
        #endregion

        public static void StopCrawler(bool nextStatus)
        {
            StopCrawling = nextStatus;
        }
        public static void SkipSite(decimal SiteId, int SkipType)
        {
            Site Site = entiti.Sites.Single<Sites>(x => x.Id == SiteId);
            if (site != null)
            {
                if (SkipType > -1 && SkipType < 4)
                    site.SkipType = SkipType;
                if (SkipType < -1 && SkipType > 4)
                    site.SkipType = 0;
            }
        }
        /// <summary>
        /// Crawls a site.
        /// </summary>
        public static void CrawlNewSite(string StartUpConfig, int IsBlog, int SkipCount, int TopCount)
        {
            TazehaContext entiti = new TazehaContext();
            EventLog.LogsBuffer.WriteLog("Beginning crawl.");
            //TazehaContext entiti = new TazehaContext();
            //Dictionary<decimal, string> sites;
            IEnumerable<Sites> sites;
            if (StartUpConfig.IndexOf("OrderByDescending") > -1)
            {
                //sites = entiti.Sites.Where<Sites>(x => (x.CrawledCount == 0 || !x.CrawledCount.HasValue) && (x.IsBlog == IsBlog || IsBlog == 2)).Select(x => new { x.Id, x.SiteUrl }).OrderByDescending(x => x.Id).Take(TopCount).ToDictionary(x => x.Id, x => x.SiteUrl);
                sites = entiti.Sites.Where<Sites>(x => (x.CrawledCount == 0 || !x.CrawledCount.HasValue) && (x.IsBlog == IsBlog || IsBlog == 2)).OrderByDescending(x => x.Id).Take(TopCount).ToList();
            }
            else
                //sites = entiti.Sites.Where<Sites>(x => (x.CrawledCount == 0 || !x.CrawledCount.HasValue) && (x.IsBlog == IsBlog || IsBlog==2)).Select(x => new { x.Id, x.SiteUrl }).OrderBy(x => x.Id).Take(TopCount).ToDictionary(x => x.Id, x => x.SiteUrl);
                sites = entiti.Sites.Where<Sites>(x => (x.CrawledCount == 0 || !x.CrawledCount.HasValue) && (x.IsBlog == IsBlog || IsBlog == 2)).OrderBy(x => x.Id).Take(TopCount).ToList();

            foreach (var site in sites)
            {
                //-----set defaulr values-----
                _externalUrls.Clear();
                _rssUrls.Clear();
                _otherUrls.Clear();
                SelfLink = 0;
                SpamCount = 0;
                //TazehaContext entiti3 = new TazehaContext();
                //Sites sitetemp = entiti.Sites.Single<Sites>(x => x.Id == site.Key);
                BaseSite = site; ;

                EventLog.LogsBuffer.WriteLog("STARTING...(" + site.Id + ") " + site.SiteUrl);
                if (StopCrawling)
                {
                    EventLog.LogsBuffer.WriteLog(">STOP CRAWLING");
                    return;
                }
                if (LinkParser.IsRestrictSite(site.SiteUrl))
                {
                    EventLog.LogsBuffer.WriteLog(">Reject " + site.SiteUrl);
                    continue;
                }
                try
                {
                    CrawlPage("HTTP://" + BaseSite.SiteUrl.ToUpper().ReplaceX("http://", ""));
                    //ThreadPool.QueueUserWorkItem(CrawlerExternalSite, new object[] { site.Value, _externalUrls });
                    CrawlerExternalSite(new object[] { site.SiteUrl, _externalUrls });
                }
                catch (Exception ex)
                {
                    EventLog.LogsBuffer.WriteLog(">Error Ln94: " + ex.InnerException);
                }
                //Thread thread = new Thread(delegate()
                //{
                //    CrawlerExternalSite();
                //});
                //thread.Start();
                //------------------insert new site in this site-----------
                try
                {
                    site.ExternalLinkCount = _externalUrls == null ? 0 : _externalUrls.Count;
                    site.CrawledCount = site.CrawledCount.HasValue ? site.CrawledCount + 1 : 1;
                    site.LastCrawledDate = DateTime.Now;
                    site.HasFeed = BaseSite.HasFeed;
                    //entiti.SaveChanges();
                }
                catch (Exception ex)
                {
                    EventLog.LogsBuffer.WriteLog(">Error feedURL:" + site.SiteUrl + " " + ex.InnerException.Message);
                }
                EventLog.LogsBuffer.WriteLog(">ENDING... " + BaseSite.SiteUrl.ToUpper());
                BaseSite = null;
            }
            try
            {
                //----SAVE ALL CHANGES----------
                entiti.SaveChanges();
            }
            catch (Exception ex)
            {
                EventLog.LogsBuffer.WriteLog(">Error SiteSaveExeption:" + " " + ex.InnerException.Message);
            }


            //--------------CHECK KARDAN MOJOOD BOODANE SITE PEYMAYESH NASHODE-----------
            int NoCrawled = 0;
            if (StartUpConfig.IndexOf("OrderByDescending") > -1)
            {
                NoCrawled = entiti.Sites.Where<Sites>(x => x.CrawledCount == 0 || !x.CrawledCount.HasValue).OrderByDescending(x => x.Id).Take(TopCount).Count();
            }
            else
                NoCrawled = entiti.Sites.Where<Sites>(x => x.CrawledCount == 0 || !x.CrawledCount.HasValue).OrderBy(x => x.Id).Take(TopCount).Count();
            if (NoCrawled > (TopCount / 2))
            {
                CrawlNewSite(StartUpConfig, IsBlog, SkipCount, TopCount);
            }
            EventLog.LogsBuffer.WriteLog("@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@/ KOLAN VARES /@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@");

        }

        public static void CrawlerExternalSite(object param)
        {
            TazehaContext entiti = new TazehaContext();
            object[] array = param as object[];
            string ParentSite = array[0].ToString();
            List<string> PexternalUrls = ((List<string>)array[1]);            
            PexternalUrls = PexternalUrls.Distinct().ToList();
            string Temstr= string.Join(",", new List<string>(PexternalUrls).ToArray());
            PexternalUrls = entiti.Sites_CheckNotIn(Temstr, ",").ToList();
            foreach (string str in PexternalUrls.ToList())
            {
                if (StopCrawling)
                {
                    EventLog.LogsBuffer.WriteLog(">STOP CRAWLING");
                    Thread.CurrentThread.Abort();
                    return;
                }
                string htmlText = string.Empty;
                Sites externalsite = new Sites();
                externalsite.SiteUrl = LinkParser.ExtractDomainNameFromURL(str.ToUpper());
                //------age urr moshkel dasht az list kharej mishavad----------
                if (string.IsNullOrEmpty(externalsite.SiteUrl.Trim()))
                    continue;

                if (LinkParser.IsRestrictSite(externalsite.SiteUrl))
                {
                    EventLog.LogsBuffer.WriteLog(">Reject " + str);
                    continue;
                }                
                htmlText = GetWebText("HTTP://" + externalsite.SiteUrl.ToUpper().ReplaceX("http://", ""));
                Page page = new Page(htmlText);
                if (string.IsNullOrEmpty(page.Content))
                {
                    EventLog.LogsBuffer.WriteLog(">Reject " + str);
                    continue;
                }
                if (!Helper.Utility.HasFaWord(page.FullTitle))
                    if (!Helper.Utility.HasFaWord(page.Description))
                        if (str.IndexOfX(".IR") < 4)
                        {
                            if (PexternalUrls.Where(x => x.ContainsX(str)).Count() > 1)
                            {
                                PexternalUrls.RemoveAll(x => x.ContainsX(str));
                            }
                            EventLog.LogsBuffer.WriteLog(">Reject " + str);
                            continue;
                        }
                        else
                            page.Title = page.Title.SubstringX(0, 150);

                externalsite.SiteTitle = string.IsNullOrEmpty(page.Title) ? page.Url.ReplaceX("http://", "").ReplaceX("www.", "") : page.Title.SubstringX(0, 150);
                externalsite.SiteTags = page.KeyWord.SubstringX(0, 300);
                externalsite.SiteDesc = page.Description.SubstringX(0, 300);
                // externalsite.IndexPageText = page.Content;
                externalsite.LinkedCount = externalsite.LinkedCount.HasValue ? externalsite.LinkedCount + 1 : 1;
                string blog = LinkParser.GetSubDomain(externalsite.SiteUrl);
                externalsite.IsBlog = string.IsNullOrEmpty(blog) ? 0 : 1;
                entiti.AddToSites(externalsite);

                //try
                //{
                //    //entiti.SaveChanges();
                //    EventLog.LogsBuffer.WriteLog(">OK NewSite " + str);
                //    ExternalLinkCount++;
                //}
                //catch (Exception ex)
                //{
                //    if (ex.Message.IndexOfX("Inner Exception") > 0)
                //    {
                //        if (ex.InnerException.Message.IndexOfX("IX_Sites_URL") > -1)
                //        {
                //            //EventLog.LogsBuffer.WriteLog(">Errror SiteExeption " + ex.InnerException);
                //            EventLog.LogsBuffer.WriteLog(string.Format(">Errror {0} {1} IX_Sites_URL {2}", BaseSite, ParentSite, str));
                //            entiti.DetectChanges();
                //        }
                //    }
                //    else
                //        EventLog.LogsBuffer.WriteLog(">Errror SiteExeption " + ex.Message);
                //}
            }
            try
            {               
                entiti.SaveChanges();
            }
            catch
            {
                entiti.DetectChanges();
            }           
        }        

        /// <summary>
        /// Crawls a page.
        /// </summary>
        /// <param name="url">The url to crawl.</param>
        public static void CrawlPage(string url)
        {
            //-------------------------------PAK BESHE HAMAN---------------
            //CrawlerEngine.Site Site = new CrawlerEngine.Sites();
            //site.SiteUrl = "http://p30eng.blogfa.com/";
            //CrawlerEngine.Crawler.BaseSite = site;
            if (!PageHasBeenCrawled(url) || SelfLink < 4)
            {
                string htmlText = GetWebText(url);
                if (string.IsNullOrEmpty(htmlText.Trim()))
                    return;
                Page page = new Page();
                page.Content = htmlText;
                page.Url = url;
                if (PageHasBeenCrawled(page))
                    return;
                else
                    _pages.Add(page);

                LinkParser linkParser = new LinkParser();
                linkParser.ParseLinks(page, url);

                //Add data to main data lists
                AddRangeButNoDuplicates(_externalUrls, linkParser.ExternalUrls);
                AddRangeButNoDuplicates(_otherUrls, linkParser.OtherUrls);
                AddRangeButNoDuplicates(_rssUrls, linkParser.RssUrls);

                foreach (string exception in linkParser.Exceptions)
                    _exceptions.Add(exception);

                //Crawl all the links found on the page.
                foreach (string link in linkParser.GoodUrls)
                {
                    string formattedLink = link;
                    try
                    {
                        formattedLink = FixPath(url, formattedLink);
                        if (formattedLink != String.Empty)
                        {
                            CrawlPage(formattedLink);
                        }
                    }
                    catch (Exception exc)
                    {
                        // linkParser.GoodUrls.Remove(link);
                        _failedUrls.Add(formattedLink + " (on page at url " + url + ") - " + exc.Message);
                    }
                }
            }
        }

        /// <summary>
        /// Fixes a path..........***............... Makes  sure it is a fully functional absolute url.
        /// </summary>
        /// <param name="originatingUrl">The url that the link was found in.</param>
        /// <param name="link">The link to be fixed up.</param>
        /// <returns>A fixed url that is fit to be fetched.</returns>
        public static string FixPath(string originatingUrl, string link)
        {
            string formattedLink = String.Empty;
            if (link.IndexOf("../") > -1)
            {
                formattedLink = ResolveRelativePaths(link, originatingUrl);
            }
            else if (originatingUrl.IndexOfX(BaseSite.SiteUrl) > -1
                && link.IndexOfX(BaseSite.SiteUrl) == -1)
            {
                if (!string.IsNullOrEmpty(link))
                    if (link[0] == '/')
                        link = link.Remove(0, 1);
                if (link.Equals(originatingUrl, StringComparison.CurrentCultureIgnoreCase))
                    return originatingUrl;
                if (originatingUrl.LastIndexOf("/") == originatingUrl.Length - 1)
                    formattedLink = originatingUrl.Substring(0, originatingUrl.LastIndexOf("/") + 1) + link;
                else
                    formattedLink = originatingUrl.Substring(0, originatingUrl.Length) + "/" + link;
            }
            else if (link.IndexOfX(BaseSite.SiteUrl) == -1)
            {
                formattedLink = BaseSite.SiteUrl + link;
            }
            else if (link.IndexOfX(LinkParser.ExtractDomainNameFromURL(originatingUrl)) > -1)
                return link;
            return formattedLink;
        }

        /// <summary>
        /// Needed a method to turn a relative path into an absolute path. And this seems to work.
        /// </summary>
        /// <param name="relativeUrl">The relative url.</param>
        /// <param name="originatingUrl">The url that contained the relative url.</param>
        /// <returns>A url that was relative but is now absolute.</returns>
        private static string ResolveRelativePaths(string relativeUrl, string originatingUrl)
        {
            string resolvedUrl = String.Empty;

            string[] relativeUrlArray = relativeUrl.Split(new char[] { '/' }, StringSplitOptions.RemoveEmptyEntries);
            string[] originatingUrlElements = originatingUrl.Split(new char[] { '/' }, StringSplitOptions.RemoveEmptyEntries);
            int indexOfFirstNonRelativePathElement = 0;
            for (int i = 0; i <= relativeUrlArray.Length - 1; i++)
            {
                if (relativeUrlArray[i] != "..")
                {
                    indexOfFirstNonRelativePathElement = i;
                    break;
                }
            }

            int countOfOriginatingUrlElementsToUse = originatingUrlElements.Length - indexOfFirstNonRelativePathElement - 1;
            for (int i = 0; i <= countOfOriginatingUrlElementsToUse - 1; i++)
            {
                if (originatingUrlElements[i] == "http:" || originatingUrlElements[i] == "https:")
                    resolvedUrl += originatingUrlElements[i] + "//";
                else
                    resolvedUrl += originatingUrlElements[i] + "/";
            }

            for (int i = 0; i <= relativeUrlArray.Length - 1; i++)
            {
                if (i >= indexOfFirstNonRelativePathElement)
                {
                    resolvedUrl += relativeUrlArray[i];

                    if (i < relativeUrlArray.Length - 1)
                        resolvedUrl += "/";
                }
            }

            return resolvedUrl;
        }

        /// <summary>
        /// Checks to see if the page has been crawled.
        /// </summary>
        /// <param name="url">The url that has potentially been crawled.</param>
        /// <returns>Boolean indicating whether or not the page has been crawled.</returns>
        private static bool PageHasBeenCrawled(string url)
        {
            foreach (Page page in _pages)
            {
                if (page.Url.Replace("/", "").Equals(url.Replace("/", ""), StringComparison.CurrentCultureIgnoreCase))
                    return true;
            }

            return false;
        }
        private static bool PageHasBeenCrawled(Page page)
        {
            foreach (Page item in _pages)
            {
                if (item.Content.Contains(page.Content.SubstringX(0, 600)))
                {
                    SelfLink++;
                    return true;
                }
            }

            return false;
        }
        /// <summary>
        /// Merges a two lists of strings.
        /// </summary>
        /// <param name="targetList">The list into which to merge.</param>
        /// <param name="sourceList">The list whose values need to be merged.</param>
        private static void AddRangeButNoDuplicates(List<string> targetList, List<string> sourceList)
        {
            foreach (string str in sourceList)
            {

                if (!targetList.Contains(str.ToUpper()))
                    targetList.AddX(str.ToUpper());
            }
        }

        /// <summary>
        /// Gets the response text for a given url.
        /// </summary>
        /// <param name="url">The url whose text needs to be fetched.</param>
        /// <returns>The text of the response.</returns>
        public static string GetWebText(string url)
        {
            HttpWebRequest request = (HttpWebRequest)HttpWebRequest.Create(url);
            request.UserAgent = "PersianFeedCrawler";

            try
            {
                WebResponse response = request.GetResponse();
                Stream stream = response.GetResponseStream();
                StreamReader reader = new StreamReader(stream);
                string htmlText = reader.ReadToEnd();
                return htmlText;
            }
            catch (Exception ex)
            {
                EventLog.LogsBuffer.WriteLog(">Error GetWebText" + ex.Message);
                return string.Empty;
            }
        }

        public static bool CrawlerProcessIsRuning()
        {
            if (BaseSite == null)
            {
                return false;
            }
            if (BaseSite.SiteUrl.Length > 4 && StopCrawling == false)
            {
                //---------------proccess now runing on server------
                return true;
            }
            return false;
        }

        #region Logging and Reporting

        private static void WriteReportToDisk(string contents)
        {
            string fileName = ConfigurationManager.AppSettings["logTextFileName"].ToString();
            FileStream fStream = null;
            if (File.Exists(fileName))
            {
                File.Delete(fileName);
                fStream = File.Create(fileName);
            }
            else
            {
                fStream = File.OpenWrite(fileName);
            }

            using (TextWriter writer = new StreamWriter(fStream))
            {
                writer.WriteLine(contents);
                writer.Flush();
            }

            fStream.Dispose();
        }

        /// <summary>
        /// Creates aaa report out of the data gathered.
        /// </summary>
        /// <returns></returns>
        private static StringBuilder CreateReport()
        {
            StringBuilder sb = new StringBuilder();

            sb.Append("<html><head><title>Crawl Report</title><style>");
            sb.Append("table { border: solid 3px black; border-collapse: collapse; }");
            sb.Append("table tr th { font-weight: bold; padding: 3px; padding-left: 10px; padding-right: 10px; }");
            sb.Append("table tr td { border: solid 1px black; padding: 3px;}");
            sb.Append("h1, h2, p { font-family: Rockwell; }");
            sb.Append("p { font-family: Rockwell; font-size: smaller; }");
            sb.Append("h2 { margin-top: 45px; }");
            sb.Append("</style></head><body>");
            sb.Append("<h1>Crawl Report</h1>");

            sb.Append("<h2>Internal Urls - In Order Crawled</h2>");
            sb.Append("<p>These are the pages found within the site. The size is calculated by getting value of the Length of the text of the response text. This is the order in which they were crawled.</p>");

            sb.Append("<table><tr><th>Page Size</th><th>Viewstate Size</th><th>Url</th></tr>");

            foreach (Page page in _pages)
            {
                sb.Append("<tr><td>");
                sb.Append(page.Size.ToString());
                sb.Append("</td><td>");
                sb.Append(page.ViewstateSize.ToString());
                sb.Append("</td><td>");
                sb.Append(page.Url);
                sb.Append("</td></tr>");
            }

            sb.Append("</table>");


            sb.Append("<h2>Internal Urls - In Order of Size</h2>");
            sb.Append("<p>These are the pages found within the site. The size is calculated by getting value of the Length of the text of the response text. This is the order in terms of total page size.</p>");

            sb.Append("<table><tr><th>Page Size</th><th>Viewstate Size</th><th>Url</th></tr>");

            List<Page> sortedList = new List<Page>();
            foreach (Page page in _pages)
            {
                if (sortedList.Count == 0)
                {
                    sortedList.Add(page);
                }
                else
                {
                    for (int i = 0; i <= sortedList.Count - 1; i++)
                    {
                        Page sortedPage = sortedList[i];

                        if (sortedPage.Size > page.Size)
                        {
                            sortedList.Insert(i, page);
                            break;
                        }
                        else if (i == sortedList.Count - 1)
                        {
                            sortedList.Add(page);
                            break;
                        }
                    }
                }
            }

            for (int i = sortedList.Count - 1; i >= 0; i--)
            {
                Page page = sortedList[i];

                sb.Append("<tr><td>");
                sb.Append(page.Size.ToString());
                sb.Append("</td><td>");
                sb.Append(page.ViewstateSize.ToString());
                sb.Append("</td><td>");
                sb.Append(page.Url);
                sb.Append("</td></tr>");
            }
            sb.Append("</table>");


            sb.Append("<h2>External Urls</h2>");
            sb.Append("<p>These are the links to the pages outside the site.</p>");

            sb.Append("<table><tr><th>Url</th></tr>");

            foreach (string str in _externalUrls)
            {
                sb.Append("<tr><td>");
                sb.Append(str);
                sb.Append("</td></tr>");
            }


            sb.Append("</table>");
            //-----------------------------------RSS with xami----------------------------------------

            sb.Append("<h2>RSS Urls</h2>");


            sb.Append("<table><tr><th>Url</th></tr>");



            foreach (string str in _rssUrls)
            {
                sb.Append("<tr><td>");
                sb.Append(str);
                sb.Append("</td></tr>");
            }

            sb.Append("</table>");

            sb.Append("<h2>Bad Urls</h2>");
            sb.Append("<p>Any bad urls will be listed here.</p>");

            sb.Append("<table><tr><th>Url</th></tr>");
            //-----------------------------------------------end-----------------

            sb.Append("</table>");

            sb.Append("<h2>Other Urls</h2>");
            sb.Append("<p>These are the links to things on the site that are not html files (html, aspx, etc.), like images and css files.</p>");

            sb.Append("<table><tr><th>Url</th></tr>");



            foreach (string str in _otherUrls)
            {
                sb.Append("<tr><td>");
                sb.Append(str);
                sb.Append("</td></tr>");
            }

            sb.Append("</table>");

            sb.Append("<h2>Bad Urls</h2>");
            sb.Append("<p>Any bad urls will be listed here.</p>");

            sb.Append("<table><tr><th>Url</th></tr>");

            if (_failedUrls.Count > 0)
            {
                foreach (string str in _failedUrls)
                {
                    sb.Append("<tr><td>");
                    sb.Append(str);
                    sb.Append("</td></tr>");
                }
            }
            else
            {
                sb.Append("<tr><td>No bad urls.</td></tr>");
            }

            sb.Append("</table>");


            sb.Append("<h2>Exceptions</h2>");
            sb.Append("<p>Any exceptions that were thrown would be shown here.</p>");

            sb.Append("<table><tr><th>Exception</th></tr>");

            if (_exceptions.Count > 0)
            {
                foreach (string str in _exceptions)
                {
                    sb.Append("<tr><td>");
                    sb.Append(str);
                    sb.Append("</td></tr>");
                }
            }
            else
            {
                sb.Append("<tr><td>No exceptions thrown.</td></tr>");
            }

            sb.Append("</table>");

            sb.Append("<h2>Css Classes</h2>");
            sb.Append("<p>These are the css classes used on the site, just in case you want to know which ones you're using and compare that with your css...</p>");

            sb.Append("<table><tr><th>Class</th></tr>");

            if (_classes.Count > 0)
            {
                foreach (string str in _classes)
                {
                    sb.Append("<tr><td>");
                    sb.Append(str);
                    sb.Append("</td></tr>");
                }
            }
            else
            {
                sb.Append("<tr><td>No classes found.</td></tr>");
            }

            sb.Append("</table>");

            sb.Append("</body></html>");
            return sb;
        }

        private static void OpenReportInIE()
        {
            object o = new object();
            InternetExplorer ie = new InternetExplorerClass();
            IWebBrowserApp wb = (IWebBrowserApp)ie;
            wb.Visible = true;

            wb.Navigate(ConfigurationManager.AppSettings["logTextFileName"].ToString(), ref o, ref o, ref o, ref o);
        }

        #endregion

      
     

  
    }
}
